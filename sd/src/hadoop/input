The Apache Hadoop project is actually an umbrella for a set of projects that are associated with solving problems the MapReduce way. The Hadoop Core project provides the MapReduce implementation and the distributed file system HDFS (Hadoop Distributed File System). Both can be used separately. In this blog we will focus on Hadoop Core project.

You may probably ask yourself: ‘Why do I need a distributed file system?’. To answer this question you need to ask your self the following question: ‘How can I access a large dataset in a fast and concurrent manner?’ One way to go is to invest a lot of money in the best hardware you can find. While this take you a few steps forward, when it comes to really large data sets, you soon hit limits again. The other alternative is to distribute your data and that is why HDFS is used in Hadoop. So each process can access each chunk of data simultaneously, while the data may be scattered across the network. That being said, to get our word counting Hadoop application started we do not have to use HDFS. We can also use the local file system.
Implementing the map function

The first thing we need to create is the map function. This map function needs to collect words (tokens) from the input file(s). The input is a body of text that contains many (possible duplicate) words. The key here is the offset from the start of the input file and the value is a token. The map function needs to transform this into an intermediate structure, so that the reduce function can easily count the unique tokens. One simple way to do is to have the token as key and the number of times it occurred (during map phase) as the value.
