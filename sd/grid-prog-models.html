<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<title>Les Grilles Informatiques</title>
<!-- metadata -->
<meta name="generator" content="S5" />
<meta name="version" content="S5 1.1" />
<meta name="presdate" content="20050128" />
<meta name="author" content="S. Genaud" />
<meta name="company" content="Université de Strasbourg" />
<!-- configuration parameters -->
<meta name="defaultView" content="slideshow" />
<meta name="controlVis" content="hidden" />
<!-- style sheet links -->
<link rel="stylesheet" href="ui/default/slides.css" type="text/css" media="projection" id="slideProj" />
<link rel="stylesheet" href="ui/default/outline.css" type="text/css" media="screen" id="outlineStyle" />
<link rel="stylesheet" href="ui/default/print.css" type="text/css" media="print" id="slidePrint" />
<link rel="stylesheet" href="ui/default/opera.css" type="text/css" media="projection" id="operaFix" />
<!-- embedded styles -->
<style type="text/css" media="all">
.imgcon {width: 525px; margin: 0 auto; padding: 0; text-align: center;}
#anim {width: 270px; height: 320px; position: relative; margin-top: 0.5em;}
#anim img {position: absolute; top: 42px; left: 24px;}
img#me01 {top: 0; left: 0;}
img#me02 {left: 23px;}
img#me04 {top: 44px;}
img#me05 {top: 43px;left: 36px;}

.tableauMedia
{
  	margin: 0px 0px 20px 30px;
	padding: 5px;
}
.legende {
	font-family: Verdana, Arial, helvetica, sans-serif;
	font-size: 10px;
	line-height: 15px;
	padding: 3px 0 0 0;
}
.legendeTitre
{
	font-family: Verdana, Arial, Helvetica, sans-serif;
	font-size: 11px;
	line-height: 16px;
	color: #333;
	font-weight: bold;
}


.important { color : red; }

table.list { border-spacing : 3px; 
             margin-left: auto; 
             margin-right : auto; /* pour centrer la table */ 
           }
tr.list    { background-color : #eee59a; font : fixed; }
th.list    { background-color : #c2ddeb; }
td.list    { background-color : #eee59a; margin : 3px; 
        font-family : courier ; font-size : 12pt;}


#listing pre { padding: 0; margin: 0; 
               background-color : #c2ddeb; 
               text-fontsize : 30%;
           border-bottom: 1px solid #000;
           border-top   : 1px solid #000;
           width: 100%; position: relative;}
</style>


<!-- S5 JS -->
<script src="ui/default/slides.js" type="text/javascript"></script>
</head>


<body>

<div class="layout">
<div id="controls"><!-- DO NOT EDIT --></div>
<div id="currentSlide"><!-- DO NOT EDIT --></div>
<div id="header"></div>
<div id="footer">
<h1>Novembre &#8226; 2010</h1>
<h2>Les Grilles Informatiques</h2>
</h2>
</div>
</div>

<div class="presentation">

<div class="slide">
<h1>Les Grilles informatiques</h1>
<h2>Modèles de programmation</h2>
<h4><a href="http://icps.u-strasbg.fr/~genaud">Stéphane Genaud</a></h4>
</div>

<!-- _______________________________  Réalité _____________________________ -->
<div class="slide">
<h1>La réalité des Grilles</h1>

<p>
Que couvre au sens large le terme "une grille" ?
</p>
<ul>
  <li>L'activité: l'utilisation faite des logiciels (e.g. calcul distribué, partage de fichier),</li>
  <li>Le matériel (e.g. des PCs de particuliers, des clusters fédérés, ...), </li>
  <li>Les utilisateurs/les fournisseurs (qui a le droit d'utiliser le système, qui doit l'administrer).</li>
</ul>

<p>
Dans chacune de ces catégories existe une grande diversité.
</p>
</div>

<!-- _______________________________ L'existant  _____________________________ -->

<div class="slide">
<h1>Catégories</h1>

<p>
Aujourd'hui, les applications distribuées à  large échelle sont variées, car
grande diversité de besoins et d'utilisations des ressources informatiques.
On peut les classer en catégories:
</p>
<ul>
<li>Calcul distribué,</li>
<li>Meta-computing,</li>
<li>Internet computing,</li>
<li>Réseaux pair-à -pair.</li>
</ul>
<p>
De nombreux projets réussis dans chacun de ces domaines
ont confirmé l'intérêt du partage de ressources.
</p>
</div>

<div class="slide">
<h1>Calcul distribué</h1>

<p>
Désigne toutes les solutions oà¹ plusieurs ordinateurs résolvent un problème ensemble,
en communiquant à  travers le réseau.
</p>
<p>
Terme générique mais utilisé surtout aujourd'hui pour désigner des projets qui utilisent des 
ressources internes à  une organisation
(les plus nombreux).
</p>
<div>
<img style="float:right" src="grid-img/ilm_yoda.png" alt="frame rendering"/> 
<p>
Mises en oeuvre très variées: de CORBA, en passant par RMI, à  MPI, déployées sur des réseau de stations, cluster ou même machine parallèle.
</p>
<div style="font-size: smaller; background-color:#ddddff;border: dashed black 1px; padding-right : 2px; padding-left : 2px; magin-right:10px;">
<b>Exemple</b>: <br/>
rendu d'images dans un film. Il faut produire au moins
20 scènes/s * 60 s * 90 min = 108 000 scènes.
Si une scène coûte 1h de calcul, il faut 12 ans de calcul.
<p>
ILM, filiale de Dreamworks a déployé
<a href="http://www.linuxjournal.com/xstatic/articles/lj/0099/6011/6011s1.html">
350 PCs pour Star Wars Episode II
</a>.
</p>
</div>
</div> <!-- bottom -->

</div>


<div class="slide">
<h1>Meta-computing</h1>

<p>
Dans les années 1990, ce terme désigne des projets d'interconnexion de super-calculateurs
pour les capacités records d'une telle installation. Exemple (1997-2004), program PACI.
<img style="float:right" src="grid-img/nsf_PACI_1998.gif" alt="Project NFS PACI"/>
</p>
<p>
Les applications déployées sont des applications <i class="important">parallèles</i>.
Les configurations exceptionnelles visent à  relever
des <em>grand défis</em>.
</p>
<!-- 
From http://gridcafe.web.cern.ch/gridcafe/Gridhistory/ancestors.html :
"metacomputing", a term that dates back to around 1990, and was used to describe projects aimed at interconnecting supercomputer centers in the US in order to combine the processing power of multiple supercomputers. Larry Smarr, a former Director of NCSA (the National Center for Supercomputing Applications in the US) is generally credited with popularizing the term.
-->


<!--
From http://www.npaci.edu/
From 1997 to 2004, the San Diego Supercomputer Center (SDSC) served as the Leading Edge Site for the National Partnership for Advanced Computational Infrastructure (NPACI), 
which partnered 41 universities and research institutions as well as international affiliates. 
Funded by the National Science Foundation (NSF), the project was led by Principal Investigator and SDSC Director, 
Dr. Fran Berman from 2001 to 2004. The NPACI program advanced science by working towards a ubiquitous, continuous and pervasive national computational infrastructure for the 21st century, building on dramatic advances in information technology to enable advances in science and engineering.
-->
</div>



<!-- _______________________________ Internet computing _____________________________ -->
<div class="slide">
<h1>Internet computing</h1>

<p>
Un cas particulier de calcul distribué. Caractéristiques:
</p>
<ul>
	  <li>L'application doit être "embarassingly parallel",</li>
	  <li>Le volume de données à  communiquer modéré,</li>
	  <li>Participation sur la base du volontariat des particuliers,</li>
	  <li>Utilise le processeur en basse priorité (<i>screen-saver</i>)</li>
</ul>
</div>

<div class="slide">
<h1>Internet computing: Exemples </h1>
<p>
<b><a href="http://setiathome.ssl.berkeley.edu/">SETI@Home</a>.</b>
Objectif: sonder les signaux électromagnétiques venus de l'espace à  la recherche d'intelligence.
Réunit aujourd'hui 400&nbsp;000 participants pour une puissante totale de 60 téraflops, soit plus que Earth Simulator (35 téraflops). 
</p>
<b><a href="http://folding.stanford.edu/french/">Folding@Home</a>.</b>
Calcul de repliement et d'agrégation de protéines. A mobilisé des milliers d'ordinateurs individuels. 
<div style="float:right">
<img style="float:right;border: dashed" alt="Un résultat de folding@home" src="grid-img/folding.gif"/>
<br/>
<div class="legende" style="float:right; width:200px">
<span class="legendeTitre">L'un des premiers résultats obtenus:
simulation d'un repliement du domaine carboxy-terminal (<i>headpiece</i>) de la villine</span>
(une protéine parmi les plus petites et les plus rapides à  se replier). Vijay Pande, Stanford University.
</div>

<div style="float:right">

Aujourd'hui, plateforme et logiciel communs à tous ces projets : <a href="http://boinc.berkeley.edu/">BOINC</a>.
La puissance de calcul totale est de l'ordre du TeraFlops, avec des centaines de milliers de machines.
</div>

</div>


</div>


</div>

<div class="handout">

<!-- From http://ditwww.epfl.ch/publications-spip/article.php3?id_article=928 -->

Le projet Seti@Home (Search for Extra-Terrestrial Intelligence) a débuté 
officiellement le 17 mai 1999. 
L'objectif est de découvrir le premier signal extraterrestre intelligent . 
Ce projet est la continuation logique du projet OZMA du NRAO (National Radio Astronomy Observatory) commencé en 1960. 
Le radiotéléscope d'Arecibo sur l'ile de Puerto Rico enregistre toutes les ondes émises par l'univers sur une plage de fréquence de Â± 2.5 MHz autour de la raie de l'hydrogène (1420 MHz). 
Cette plage de fréquence a été choisie, car elle représente une trouée dans la brume radio de l'univers, on dit qu'elle constitue le trou d'eau (waterhole).
En 3 ans, la durée du projet, le radiotélescope d'Arecibo a couvert trois fois la partie du ciel qui lui est visible, représentant 39 Tbytes de données.


<h2>Des données parfaitement indépendantes</h2>

Les observations recueillies par le radiotélescope peuvent être séparées en bandes de fréquences indépendantes et chacune d'entre elles décomposées en petits paquets à  traiter, indépendants les uns des autres. 
Le traitement consiste en une décomposition FFT (Fast Fourrier Transform) suivie par une transformée inverse. 
L'application peut donc traiter chaque paquet de données de faà§on indépendané.
Ici, la granularité choisie est de 10 KHz, de manière à  ce que ces paquets soient traitables par un ordinateur personnel. 
Une architecture client-serveur est donc parfaitement adaptée: 
le serveur distribue les paquets aux clients, ces derniers les analysent et renvoient les résultats au serveur.


</div>


<div class="slide">
<h1>Stockage distribué</h1>

<p>
Un catégorie de besoin est l'espace de stockage distribué.
Exemple: projet <a href="http://www.edg.org/">DataGrid</a> de la CE pour le LHC (fini mars 2004).

</p>
<p>
Une des applications qui a changé la perception de l'usage possible des réseaux
est l'application de partage de fichiers, dont l'initiateur fut Napster.
</p>
<p>
Ont suivi des systèmes beaucoup plus décentralisés:
<a href="http://www.gnutella.com/">Gnutella</a>,
Kaaza,
<a href="http://www.edonkey2000.com">Overnet (eDonkey)</a>,
<a href="http://www.bittorrent.com/">Bittorent</a>, ...
</p>

<p>
Ces systèmes ont prouvé l'efficacité du concept <span class="important">pair à  pair</span>.
</p>

</div>


<div class="slide">
<h1>Les plate-formes matérielles</h1>

<p>
Les <span class="important">ressources matérielles</span>
nécessaires varient selon l'activité. Exemples:
</p>
<ul>
  <li>Les grilles de type <i>Internet computing</i> requièrent de très nombreuses et puissantes CPU, mais n'ont pas besoin d'un réseau très efficace =&gt; PC de particuliers.</li>
  <li>Les simulations numériques requierent souvent beaucoup de mémoire et de bon réseaux (communications à  chaque pas de temps) =&gt; machines puissantes sur réseau protégé.</li>

</ul>
<p>
De même que les activités, les modèles de programmation utilisés ne sont pas les mêmes selon les plate-formes matérielles.
</p>
<p>
Recensons quelques catégories de Grilles au sens matériel.
</p>
</div>




<div class="slide">
<h1>Grille "institutionnelle"</h1>
<img style="float:right" 
     src="grid-img/grid_lanlqnov.jpg" 
     alt="weblike sharing"
     width="500"/> 
<ul>
<li>Des institutions partagent des resources onéreuses (clusters,
super-calculateurs, etc).</li>
<li>Le nombre de machines installées est faible.</li>
<li>La fiabilité des noeuds est grande.</li>
<li>Les utilisateurs sont clairement identifiés.</li>
</ul>

</div>

<div class="slide">
<h1>Desktop grid</h1>

<ul>
<li>Les éléments de la grille sont peux coûteux individuellement (e.g. PC).</li>
<li>Le nombre de machines installées est grand.</li>
<li>La fiabilité des noeuds est faible (parfois, volatilité extrême)</li>
<li>Les utilisateurs ne sont pas toujours clairement identifiés.</li>
</ul>


</div>


<!-- _______________________________ Définitions _____________________________ -->
<div class="slide">
<h1>Définition d'une Grille</h1>

<p>Comment définir ce qui doit répondre à des besoins et à des exigences 
techniques si différentes ? Dégageons des points communs :
</p>
<ul>
<li>L'étendue géographique du système est internationale.</li>
<li>Le partage de ressource est généralisé à  n'importe quel 
élément d'un système informatique (e.g. processeur, disque, workbench, fichier, ...).</li>
<li>Les ressources de même type sont probablement hétérogènes (e.g. processeur, OS).</li>
<li>Les utilisateurs du système forment une communauté (avec des contraintes d'identifications plus ou moins fortes).</li>
</ul>
</div>

<!--
<ul>
	<li>Besoin de puissance de calcul et espace de stockage de nombreuses machines 
	sous-exploitées : amélioration du rendement des ressources.
	<ul>
		  <li>Calcul de type "embarassingly parallel" (SEITI@Home, RC5,...),</li>
		  <li>Calcul d'applications parallèles (applications MPI, ...).</li>
	</ul>
	</li>
	<li>Besoin d'accéder/fournir des services distribués : faciliter les transactions client/fournisseur
	<ul>
		  <li>Proposer des applications en ligne,</li>
		  <li>Concevoir des applications distribuées pérennes.</li>
	</ul>
	</li>
</ul>
</div>
-->
<!-- _______________________________ Conclusion _____________________________ -->
<div class="slide">
<h1>Conclusion</h1>

<p>
Il est difficile de donner une définition unique des grilles car
les besoins et les contraintes des activités dans le domaine
de l'informatique distribuée sont différents.
</p>
<p>
Cependant, convergence dans l'idée de:
</p>
<ul>
        <li>mutualisation généralisée des ressources (prêter/emprunter),</li>
        <li>transparence dans l'accès aux ressources,</li>
        <li>formation de communautés d'utilisateurs,</li>
        <li>augmentation de la puissance du système (calcul/stockage).</li>
</ul>        
</div>





<!-- Part : Modèles de programmation -->

<div class="slide">
 <h1>Comment programmer pour de telles architectures</h1>
 Problèmes ouverts
 <ul>
 <li> Portabilité, Interopérabilité : indépendance architecture
 <li> Adaptivité : adaptation à la charge 
 <li> Découverte : programmes capables de trouver de nouvelles ressources 
 <li> Performance :
  	<ul>
	<li> latence réseau : gros grain
	<li> ordonnancement : conflits potentiels sur ressources
	<li> variance de la disponibilité : qualité de service
	</ul>
 </ul>
</div>

<div class="slide">
 <h1>Problèmes ouverts (2)</h1>
 <ul>
 <li> Tolérance aux pannes : résister à la <em>volatilité</em>. </li>
 <li> Sécurité : 
      <ul>
  	<li> encapsulation des codes exécutés (sandboxing)</li>
	<li>vérifier la validité des résultats</li>
      </ul>
 </li>
 </ul>
</div>

<div class="slide">
<h1>Modèles de programmation</h1>
 <ul>
 <li> Modèles à passages de messages : MPI et variantes </li>
 <li> Modèles RPC (ou client/serveur) : CORBA, RMI, GridRPC, ... </li>
 <li> Modèles Pair-à-pair : Chord, JXTA, ... </li>
 </ul>
</div>

<div class="slide">
<h1>Modèles RPC</h1>

<ul>
<li>Vient du monde système distribué : le client/serveur.</li>
<li> Utilisation de technologies répandues : RPC, CORBA, RMI sont
bien connus.</li>
<li> Permet une distribution "incrémentale" du code : on peut ne
remplacer qu'une procédure par un appel distant.</li>
<li> Les applications ne peuvent rivaliser en performance
      avec des applications parallèles : on n'a pas un graphe complet
	de communication entre processus.
</li>
</ul>

</div>


<div class="slide">
<h1>Exemple GridRPC</h1>
<i>A Remote Procedure Call API for Grid Computing</i>, 
    Seymour,Nakada,Matsuoka,Dongarra,Lee,Casanova, 2002.
	 
<div id="listing">
<pre>
main() { 
   grpc_function_handle_t h;
   grpc_initialize(argv[1]);
   grpc_function_handle_default(&amp;h,"mat/mmul");
   if (grpc_call(&amp;h,N,A,B,C) != GRPC_ERROR) {
     printf("error\n");
   }
</pre>
</div>
</div>

<div class="slide">
<h1>Exemple GridRPC (2)</h1>

aussi :
<div id="listing">
<pre>
grpc_function_handle_init()
                /* lien vers un serveur precis*/
grpc_function_handle_destruct()
grpc_finalize()

grpc_call_async()   /* appel non bloquant*/
grpc_probe()        /* test si l'appel est fini*/
grpc_wait()         /* attend fin d'un appel */
grpc_cancel()       /* annule l'appel */
</pre></div>
</div>

<div class="slide">
<h1>Exemple GridRPC (3)</h1>
<ul>
<li> Le serveur peut réaliser une implémentation parallèle</li>

<li> Nécessite un langage d'interface (IDL)</li>
</ul>
<div class="listing">
<pre>
Module mat;
 
Define mmul(IN int N, 
            IN double A[N*N], 
            IN double B[N*N], 
            OUT double C[N*N])
"matmul"
Required "mmul_lib.o"
Calls "C" mmul(N, A, B, C);
</pre>
</div>
</div>

<div class="slide">
<h1>Modèle RPC : problèmes</h1>

<ul>
<li> performances : goulot d'étranglement constitué
      par l'interface de communication unique de l'objet
	serveur
	
<li> trouver le service adéquat d'après l'appel dans le code client 
      (interface repository distribué)

<li> trouver le <b>meilleur</b> service (le plus rapide,
      le moins cher, ....)

</ul>

</div>


<div class="slide">
<h1>Modèle à passages de messages</h1>

<ul>
  <li> Avantages 
	<ul>
	<li> portage immédiat d'un code pour machine
	      parallèle vers une grille (e.g. MPICH-g2)</li>
      <li> toujours graphe complet de communication
	      entre processus (pas de goulot d'étranglement
		d'un point de vue logique)</li>
	</ul>
  </li>
  <li> Problèmes
	<ul>
	<li> la conception de la parallélisation dépend de l'architecture
	      cible</li>
	<li> la grille est composée de liens réseaux et processeurs hétérogènes</li>
	<li> les performances de l'application parallèle seront faibles</li>
	</ul>
   </li>
</ul>
</div>


<div class="slide">
<h1> MPI pour Grille</h1>

Amélioration de la librairie de communication
<ul> 
<li> Réglage fin des buffers de tra/nsmission (e.g. globus I/O)</li>
<li> Utilisation d'un vendor MPI (optimisé) quand disponible</li>
<li> Opérations de communication collectives adaptées à la topologie physique :
      essayer de moins utiliser les liens les plus lents.
	(e.g. MPICH-G2, Magpie, PACXMPI)
	MPICH-G2 différencie 4 niveaux : WAN-TCP,LAN-TCP,intra,vendor MPI </li>
</ul>
</div>



<div class="slide">
<h1>Modèle pair-à-pair</h1>

Chaque noeud de la grille dispose d'une certaine autonomie
<ul>
<li>Avantages 
	<ul>
  	  <li> protocoles de découverte inhérent à l'architecture</li>
	  <li> tolérance aux pannes plus facilement réalisable </li>
        <li> maintenance logiciel/matériel plus simple</li>
        <li> tradition de prise en compte des pare-feux </li>
	</ul>
</li>
<li> Problèmes
	<ul>
	<li> les performances de l'application parallèle seront faibles</li>
	</ul>
</li>
</ul>
</div>


</div> <!-- ********** div final ************* -->

</body>
</html>
