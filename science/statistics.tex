% Created 2012-09-16 Sun 18:07
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\providecommand{\alert}[1]{\textbf{#1}}

\title{Statistics One : Andrew Conway, Princeton}
\author{Stephane Genaud}
\date{\today}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs Org-mode version 7.8.11}}

\begin{document}

\maketitle

\setcounter{tocdepth}{3}
\tableofcontents
\vspace*{1cm}

\section{Branches of Statistics: Descriptive and Inferential}
\label{sec-1}
\subsection{Descriptive Statistics -> Experiment (L1S1)}
\label{sec-1-1}
\subsubsection{Randomized}
\label{sec-1-1-1}

    Define : 
\begin{itemize}

\item a sample of the population
\label{sec-1-1-1-1}%

\item the \textbf{dependent variables} :\\
\label{sec-1-1-1-2}%
the characteristics to be measured, 
     e.g does (yes or no) a patient has been injected the vaccine
         sex of the observed person (male, female)

\item the \textbf{independent variable} :\\
\label{sec-1-1-1-3}%
the measure, e.g what level of anticorps does a patient have
      e.g score on a test of spatial reasoning

\item counfound\\
\label{sec-1-1-1-4}%
avoid any bias in the experiment, e.g double-blind for polio
\end{itemize} % ends low level
\subsubsection{Causality}
\label{sec-1-1-2}

    compare results in the \textbf{control group} and the administered
\subsubsection{Ways of describing}
\label{sec-1-1-3}
\begin{itemize}

\item Histograms (L2S1)\\
\label{sec-1-1-3-1}%
show an entire distribution
     example : Wine testing
     30 experts rated overall quality of 4 different wine (scale 0-10)
     Example shows : rectangle, positive/negative skew (distribution +gros effectif d'abord/après)
     

\item Summary Stastistics (L2S2)
\label{sec-1-1-3-2}%
\begin{itemize}

\item Important concepts\\
\label{sec-1-1-3-2-1}%
Central tendency (mean,median,mode) (mode = the score that happens most often
      Variability (std dev, variance)
      Skew
      Kurtosis
      Example in R: `describe(ratings)', where rating the distribution
      Example for variability : Jeremy Lin, basket ball player
          arrray : points per game, 
                   X-M : deviation wrt mean, 
                   (X-M)$^2$: square the deviation so that we can sum the deviation and divie by number of matches.
          M = mean = 22.7 = $\sum$ X / N
          SD = std dev = 9.6
          SD$^2$ = variance = 92.21 = ($\sum$ (X-M)$^2$)/ N 
          (also known as MS : Mean Squares)

\end{itemize} % ends low level

\item Tools for inferential statistics (L2S3)
\label{sec-1-1-3-3}%
\begin{itemize}

\item Important Concepts
\label{sec-1-1-3-3-1}%
\begin{itemize}

\item The normal distribution\\
\label{sec-1-1-3-3-1-1}%
bell shape, symetrical
       example: body temperature
                wand measurement : M=100.06, SD=.71


\item Z-scores\\
\label{sec-1-1-3-3-1-2}%
A standardized unit of measurement : convert ``raw'' score to z-score:
       Z = (X-M)/SD 

\item Percentile rank\\
\label{sec-1-1-3-3-1-3}%
def: the percentage of scores <- a given score
       e.g body temperature: mean =100.06, std-0.71
           mine=100.77, what the percentile rank ?
       Z=(100.77§100.06)/.71 = 1
\begin{itemize}
\item Calculus of the area under the curve up to x-axis 100.77
\item or look at the Z-table : see 34,1\% for std=1, so my percentile rank = 84.1\%
\end{itemize}

\item Probability\\
\label{sec-1-1-3-3-1-4}%
proba and normal distribution:
       if a choose a student at random, proba that his temperature >= 100.6 ?
       P(X>100.6) = .5
       P(X>100.77)=.159
       P(X>103) < .01

\item Inferential Statistics\\
\label{sec-1-1-3-3-1-5}%
Assume a normal distrib
\begin{itemize}
\item assume certain values, such as the mean
\item conduct an experiment
\item do the assumptions hold ?
\end{itemize}
       Safe to assume a normal distrib ???
\begin{itemize}
\item what are you trying to measure
\item what is the construct ?
\item how do you operationalize the construct (see lecture Measurement !)
\end{itemize}
      

\end{itemize} % ends low level
\end{itemize} % ends low level
\end{itemize} % ends low level
\subsection{Inferential Statistics -> Observational Study (L1S2)}
\label{sec-1-2}
\subsubsection{Correlation}
\label{sec-1-2-1}
\subsubsection{Quasi-independent variables}
\label{sec-1-2-2}
\section{Correlation (L4)}
\label{sec-2}
\subsection{Correlation Examples (L4S1)}
\label{sec-2-1}

   def: a statistical procedure to measure and describe the relationship between 2 variables
   can range [-1;1]. -1 negative correlation, 1 perfect correlation. 
   E.g working memory capacity (X) is strongly correlated with SAT score (Y)
   Graphically : scatterplot
   In R : plot(X\~{}Y) (X on the y-axis, Y on the x-axis)

   Caution about correlation: 
\begin{itemize}
\item accuracy of the prediction will depend on
     magnitude of the correlation => which depends on the reliability of X and Y, and sampling (random and representative ?)
\item validity of the prediction : correlation is a \textbf{sample} statistics => does not apply to an individual
\end{itemize}

   Example: Intelligence testing \& WW1. Develop an aptitude test:
\begin{itemize}
\item multiple choice and short§answer questions (ASVAB today)
\item R. Yerkes argued that ``native intellectual ability'' was unaffected by culture
\end{itemize}
   Statistical analysis to support/refute claim ?
   Anwser: observe difference in predictibility = correlation.
           Take two groups: officers and soldiers, and observe if the test is predictive on the job.

   Example: Baseball. 
\subsection{Correlation Calculations (L4S2)}
\label{sec-2-2}
\subsubsection{Correlation coefficient $r$}
\label{sec-2-2-1}

  (aka \emph{Pearson product-moment correlation coef})
\begin{itemize}
\item $r$ = the degree to which X and Y vary together, relative to the degree X and Y vary independently
\item $r = covariance(X,Y) / variance(X,Y)$
\end{itemize}
  Fomulae for $r$: 2 different ways:
\begin{itemize}
\item Raw score formula
\item Z-score formula
\end{itemize}
\subsubsection{New concept : SP : Sum of Cross Products}
\label{sec-2-2-2}

\begin{itemize}
\item Review: Sum Squares: $SS = \sum_i ( X_i - M)^2$
\item SP:
\end{itemize}
    -- calculate deviation for X and Y \\
    -- for each subject, multiply the deviation scores of $X$ and $Y$:\\
       \$(X-M$_X$) \texttimes{} (Y - M$_Y$)\$\\
    -- then sum the cross-products: 
       $SP=\sum_{i=1}^n (X-M_X) \times (Y - M_Y)$
\subsubsection{Formula for $r$}
\label{sec-2-2-3}

\begin{itemize}
\item Using Raw score :  $r = SP_{X,Y} / \sqrt{SS_X \times SS_Y}$
\item Using Z-score : $r = \frac{\sum_{i-1}^N ({Z_x Z_Y})}{N}$
\end{itemize}
\subsubsection{Variance and Covariance}
\label{sec-2-2-4}

\begin{itemize}
\item Variance = SP /N
\item Covariance= SP /N
\item Correlation is standardized covariance (range -1 to 1)
\end{itemize}
\subsection{Interpretation of Correlations (L4S3)}
\label{sec-2-3}
\subsubsection{Validity of a correlation-based argumentation}
\label{sec-2-3-1}

    Assumptions behind correlation analyses:
\begin{itemize}
\item normal distributions for X and Y. 
      Detect violation by plotting, adn descriptive statistics.
\item linear relationship between X and Y
      Detect violation by looking at the scatter plot, or more precise : residuals
\item Homoskadesticity
      In a scatterplot the distance between a dot and the regression line reflects the amount
      of prediction error = \textbf{residual}.
      Homoskadesticity : def: the residuals are not a function of the values of X
      (residuals look like random values).
\end{itemize}
\subsubsection{Reliability of a correlation}
\label{sec-2-3-2}

    If i go to another sample, will i have the same correlation ?
\begin{itemize}
\item one approach is NHST : Null Hypothesis Significance Testing
\end{itemize}
    Consider :
\begin{itemize}
\item $H_0$ =  null hypothesis, e.g r=0
\item $H_A$ = alternative hypothese, e.g r>0
\end{itemize}
    NHST
    Assume $H_0$ is true, then calculate the probability of observing data with
    these caracteristics, given $H_0$ is true
\begin{itemize}
\item Thus, $p = P(D|H_0)$
\item if $p < \alpha$ then reject $H_0$ else retain $H_0$.
\end{itemize}


\begin{center}
\begin{tabular}{lll}
 action       &  retain H$_0$  &  reject H$_0$  \\
\hline
 H$_0$ true   &  correct       &  false alarm   \\
 H$_0$ false  &  type II err   &  correct       \\
\end{tabular}
\end{center}




\begin{center}
\begin{tabular}{ll}
 retain H$_0$  &  reject H$_0$  \\
\end{tabular}
\end{center}


------------+------------+--------------

\begin{center}
\begin{tabular}{lll}
 H$_0$ true  &  $p=1-\alpha$  &  \$p=$\alpha$\$  \\
\end{tabular}
\end{center}


------------+------------+--------------

\begin{center}
\begin{tabular}{lll}
 H$_0$ false  &  $p=\beta$  &  $p=1-\beta$  \\
              &  (Miss)     &               \\
\end{tabular}
\end{center}


\begin{itemize}
\item $p = P(D|H_0)$
\item Given that the null hypothesis is true, the probability
      of these, or more extreme data, is p.
      \textbf{NOT} : the probabilit of the null hypothesis being true is p.
      In other word :\\
      \$p = P(D|H$_0$) $\neq$ $p = P(H_0 | D)$
\end{itemize}
  
\subsubsection{NHST application}
\label{sec-2-3-3}

    NHST can be applied to:
\begin{itemize}
\item r : is the correlation significantly different from 0
\item r1 vs. r2 : is one correlation significantly larger than another
\end{itemize}
\subsection{Reliability and Validity of Correlation (L5S1)}
\label{sec-2-4}
\subsubsection{Reliability}
\label{sec-2-4-1}

   Classical test theory
\begin{itemize}
\item raw scores (X) are not perfect
\item they are influenced by bias and chance error
\item In a perfect world, we would obtain a ``true'' score
     X = true score + bias + error
\end{itemize}
   A measure (X) is considered to be reliable as it approaches the true score
   Methods to estimate reliablility
\begin{itemize}
\item test / re-test\\
exemple measure temp body of everyone twice: X1 and X2\\
     However, if the bias is uniform, we wont't detect it
\item parallel tests
     Measure temp body with the wand (X1) and oral thermometer (X2)\\
     The correlation would reveal a bias of the wand
\item inter-item estimates
     Most commonly used in social sciences\\
     Example: suppose a 20-item survey is designed to measue extraversion
\begin{itemize}
\item randomly select 10 items to get subset A (X1)
\item the other 10 items become subset B (X2)
\item the correlation between X1 and X2 is an estimate of the reliability
\end{itemize}
\end{itemize}
\subsubsection{Validity}
\label{sec-2-4-2}

    What is a construct?\\ 
    An ``object''  that is not directly observable
\begin{itemize}
\item as opposed to ``real'' observable object
\item example, ``intelligence'' is a construct
\end{itemize}
    How do we operationalize a construct?\\
    The process of defining the conostruct to make it observalbke and quantifiable\\
\begin{itemize}
\item Example: intelligence tests
\end{itemize}

    Construct Validity
\begin{itemize}
\item Example: construct: verbal ability in children\\
one way to operationalize: vocabulary test
\item content validity: 
      does the test consists of words should know
\item convergent validity
      Does the test correlate with other, established measures of verbal ability?
      For example, reading comprehension
\item divergent validity
      Does the test correlates less with measures designed in a test of different type of ability?
      For example, spatial reasoning.
\item nomological validity
      Are the scores on the test consistent with more general theories, for example, of child development and neuroscience
      For example, a child with neural disease should have smaller scores
\end{itemize}
\subsection{Sampling (L5S2)}
\label{sec-2-5}
\subsubsection{Sampling error}
\label{sec-2-5-1}

    Example: Wine testing: 
\begin{itemize}
\item suppose a population certified experts,  N=300
\item and suppose the ratings for RedTruck are normally distributed in te population
\end{itemize}
    In that case, M=5.5 and SD=2.22 for N=300
    Actually, observed was M=5.93 and SD=2.45 for N=30 \\
    Now, take a random sample of N=100 : M=5.47 and Sd=2.19\\
    For a sample of N=10, we could have a large sampling error, M=6, and SD=1.7   

    The sampling error is the difference between the sample and the population.
\begin{itemize}
\item \textbf{Problem !}: we typically do not know the population parameters.
\item So how do we estimate the sampling error ?
\end{itemize}

    Clearly, depends 
\begin{itemize}
\item on the size of the sample
\item on the variance in the population
\end{itemize}
\subsubsection{Standard error}
\label{sec-2-5-2}

    Standard error is an estimate of amount of sampling error
\begin{itemize}
\item $SE = \frac{SD}{\sqrt{N}}$, where SD: std dev of the sample, N: size of the sample
\end{itemize}
\section{R}
\label{sec-3}
\subsection{Install packages}
\label{sec-3-1}

   From console: 

\begin{verbatim}
> install.package("pschy")
> library(psych)
> search() // list loaded pacakges
\end{verbatim}
\subsection{Script}
\label{sec-3-2}

   Example: wine testing (file )

\begin{verbatim}
Ratings <- read.table("stats1_ex01.txt",header = T) # 1st line = row names
> class(ratings)
  [1] "data.frame"
> names(ratings)
[1] "RedTruck" "WoopWoop" "HobNob"   "FourPlay"
hist(ratings$RedTruck)
# --> plots histo
layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
hist(ratings$RedTruck, xlab = "Ratings", ylab="Number", main="RedTruck")
hist(ratings$HobNob, xlab = "Ratings", ylab="Number", main="HobNob")
hist(ratings$FourPlay, xlab = "Ratings", ylab="Number", main="FourPlay")
hist(ratings$WoopWoop, xlab = "Ratings", ylab="Number", main="WoopWoop")
describe(ratings)  # from the 'psych' package, 
summary(ratings)
\end{verbatim}

\end{document}
