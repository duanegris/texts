Statistics One : Andrew Conway, Princeton

* Branches of Statistics: Descriptive and Inferential
** Descriptive Statistics -> Experiment (L1S1)
*** Randomized
    Define : 
**** a sample of the population
**** the *dependent variables* : 
     the characteristics to be measured, 
     e.g does (yes or no) a patient has been injected the vaccine
         sex of the observed person (male, female)
**** the *independent variable* :
     the measure, e.g what level of anticorps does a patient have
      e.g score on a test of spatial reasoning
**** counfound
     avoid any bias in the experiment, e.g double-blind for polio
*** Causality
    compare results in the *control group* and the administered

*** Ways of describing
*** Histograms (L2S1)
     - show an entire distribution
     - example : Wine testing
     - 30 experts rated overall quality of 4 different wine (scale 0-10)
     - Example shows : rectangle, positive/negative skew (distribution +gros effectif d'abord/après)
     
*** Summary Stastistics (L2S2)
**** Important concepts
      The four moments of the mean are:
      - Central tendency (mean,median,mode) (mode = the score that happens most often
      - Variability (std dev, variance)
      - Skew
      - Kurtosis
      Example in R: 'describe(ratings)', where rating the distribution
      Example for variability : Jeremy Lin, basket ball player
          arrray : points per game, 
                   X-M : deviation wrt mean, 
                   (X-M)^2: square the deviation so that we can sum the deviation and divide by number of matches.
          M = mean = 22.7 = \sum X / N
          SD = std dev = 9.6
          SD^2 = variance = 92.21 = (\sum (X-M)^2)/ N 
          (also known as MS : Mean Squares)

*** Tools for inferential statistics (L2S3)
**** Important Concepts
***** The normal distribution
       bell shape, symetrical
       example: body temperature
                wand measurement : M=100.06, SD=.71

***** Z-scores
       - A standardized unit of measurement : convert "raw" score to z-score:
       - Z = (X-M)/SD
       - This 
***** Percentile rank
       def: the percentage of scores <- a given score
       e.g body temperature: mean=100.06, std=0.71
           mine=100.77, what the percentile rank ?
       Z=(100.77-100.06)/.71 = 1
       - Calculus of the area under the curve up to x-axis 100.77
       - or look at the Z-table : see 34,1% for std=1, so my percentile rank = 84.1%
***** Probability
       proba and normal distribution:
       if a choose a student at random, proba that his temperature >= 100.6 ?
       P(X>100.6) = .5
       P(X>100.77)=.159
       P(X>103) < .01
***** Inferential Statistics
       Assume a normal distrib
       - assume certain values, such as the mean
       - conduct an experiment
       - do the assumptions hold ?
       Safe to assume a normal distrib ???
       - what are you trying to measure
       - what is the construct ?
       - how do you operationalize the construct (see lecture Measurement !)
      

** Inferential Statistics -> Observational Study (L1S2)
*** Correlation
*** Quasi-independent variables

* Correlation (L4)
** Correlation Examples (L4S1)
   def: a statistical procedure to measure and describe the relationship between 2 variables
   can range [-1;1]. -1 negative correlation, 1 perfect correlation. 
   E.g working memory capacity (X) is strongly correlated with SAT score (Y)
   Graphically : scatterplot
   In R : plot(X~Y) (X on the y-axis, Y on the x-axis)

   Caution about correlation: 
   - accuracy of the prediction will depend on
     magnitude of the correlation => which depends on the reliability of X and Y, and sampling (random and representative ?)    
   - validity of the prediction : correlation is a *sample* statistics => does not apply to an individual

   Example: Intelligence testing & WW1. Develop an aptitude test:
   - multiple choice and short§answer questions (ASVAB today)
   - R. Yerkes argued that "native intellectual ability" was unaffected by culture
   Statistical analysis to support/refute claim ?
   Anwser: observe difference in predictibility = correlation.
           Take two groups: officers and soldiers, and observe if the test is predictive on the job.

   Example: Baseball. 

** Correlation Calculations (L4S2)
*** Correlation coefficient $r$
  (aka /Pearson product-moment correlation coef/)
  - $r$ = the degree to which X and Y vary together, relative to the degree X and Y vary independently
  - $r = covariance(X,Y) / variance(X,Y)$
  Fomulae for $r$: 2 different ways:
  - Raw score formula
  - Z-score formula

*** New concept : SP : Sum of Cross Products
    - Review: Sum Squares: $SS = \sum_i ( X_i - M)^2$
    - SP: 
    -- calculate deviation for X and Y \\
    -- for each subject, multiply the deviation scores of $X$ and $Y$:\\
       $(X-M_X) \times (Y-M_Y)$\\
    -- then sum the cross-products: 
       $SP=\sum_{i=1}^n (X-M_X) \times (Y-M_Y)$
*** Formula for $r$
    - Using Raw score :  $r = SP_{X,Y} / \sqrt{SS_X \times SS_Y}$
    - Using Z-score : $r = \frac{\sum_{i-1}^N ({Z_x Z_Y})}{N}$
*** Variance and Covariance
    - Variance = MS (mean square) = SS/N
    - Covariance= SP /N
    - Correlation is standardized covariance (range -1 to 1) 
** Interpretation of Correlations (L4S3)
*** Validity of a correlation-based argumentation
    Assumptions behind correlation analyses:
    - normal distributions for X and Y. 
      Detect violation by plotting, adn descriptive statistics. 
    - linear relationship between X and Y
      Detect violation by looking at the scatter plot, or more precise : residuals
    - Homoscedasticity
      In a scatterplot the distance between a dot and the regression line reflects the amount
      of prediction error = *residual*.
      /homoscedasticity/ : def: the residuals are not a function of the values of X
      (residuals look like random values).
*** Reliability of a correlation
    If i go to another sample, will i have the same correlation ?
    - one approach is NHST : Null Hypothesis Significance Testing
    Consider :
    - $H_0$ =  null hypothesis, e.g r=0
    - $H_A$ = alternative hypothese, e.g r>0
    NHST
    Assume $H_0$ is true, then calculate the probability of observing data with
    these caracteristics, given $H_0$ is true
    - Thus, $p = P(D|H_0)$
    - if $p < \alpha$ then reject $H_0$ else retain $H_0$. 

#+BEGIN_TABLE
| action    | retain H_0  | reject H_0  |
|-----------+-------------+-------------|
| H_0 true  | correct     | false alarm |
| H_0 false | type II err | correct     |
#+END_TABLE

#+BEGIN_TABLE
|           | retain H_0 |  reject H_0 |
------------+------------+--------------
| H_0 true | $p=1-\alpha$ | $p=\alpha$ |
------------+------------+--------------
| H_0 false | $p=\beta$ | $p=1-\beta$ |
|           | (Miss)    |             |
#+END_TABLE


    - $p = P(D|H_0)$
    - Given that the null hypothesis is true, the probability
      of these, or more extreme data, is p.
      *NOT* : the probability of the null hypothesis being true is p.
      In other word :\\
      $p = P(D|H_0) \neq $p = P(H_0 | D)$
  
*** NHST application
    NHST can be applied to:
    - r : is the correlation significantly different from 0
    - r1 vs. r2 : is one correlation significantly larger than another 

** Reliability and Validity of Correlation (L5S1)
*** Reliability
   Classical test theory
   - raw scores (X) are not perfect
   - they are influenced by bias and chance error
   - In a perfect world, we would obtain a "true" score
     X = true score + bias + error
   A measure (X) is considered to be reliable as it approaches the true score
   Methods to estimate reliablility
   - test / re-test\\
     exemple measure temp body of everyone twice: X1 and X2\\
     However, if the bias is uniform, we wont't detect it
   - parallel tests
     Measure temp body with the wand (X1) and oral thermometer (X2)\\
     The correlation would reveal a bias of the wand
   - inter-item estimates
     Most commonly used in social sciences\\
     Example: suppose a 20-item survey is designed to measue extraversion
     - randomly select 10 items to get subset A (X1)
     - the other 10 items become subset B (X2)
     - the correlation between X1 and X2 is an estimate of the reliability

*** Validity
    What is a construct?\\ 
    An "object"  that is not directly observable
    - as opposed to "real" observable object
    - example, "intelligence" is a construct
    How do we operationalize a construct?\\
    The process of defining the conostruct to make it observalbke and quantifiable\\
    - Example: intelligence tests

    Construct Validity
    - Example: construct: verbal ability in children\\
      one way to operationalize: vocabulary test
      
    - content validity: 
      does the test consists of words should know

    - convergent validity
      Does the test correlate with other, established measures of verbal ability?
      For example, reading comprehension

    - divergent validity
      Does the test correlates less with measures designed in a test of different type of ability?
      For example, spatial reasoning.

    - nomological validity
      Are the scores on the test consistent with more general theories, for example, of child development and neuroscience
      For example, a child with neural disease should have smaller scores

** Sampling (L5S2)
*** Sampling error
    Example: Wine testing: 
    - suppose a population certified experts,  N=300
    - and suppose the ratings for RedTruck are normally distributed in the population
    In that case, M=5.5 and SD=2.22 for N=300
    Actually, observed was M=5.93 and SD=2.45 for N=30 \\
    Now, take a random sample of N=100 : M=5.47 and Sd=2.19\\
    For a sample of N=10, we could have a large sampling error, M=6, and SD=1.7   

    The sampling error is the difference between the sample and the population.
    - *Problem !*: we typically do not know the population parameters.
    - So how do we estimate the sampling error ?

    Clearly, depends 
    - on the size of the sample
    - on the variance in the population
*** Standard error
    Standard error is an estimate of amount of sampling error
    - $SE = \frac{SD}{\sqrt{N}}$, where SD: std dev of the sample, N: size of the sample

***** Probability Histogram
    Let us consider a normal distribution of samples. 
    We call it a /Prbability Histogram/
    - A distribution of sample means
    - Assume we took multiple samples of the same size and then
      plotted all the sample means
      -- N=10
      -- N=30
      -- N=100
      Standard error is the distance of one standard deviation higher or lower 
      in the distribution of sample means.

**** Distribution of the samples means
     - the mean of the distribution means should be the same as the population of individuals.
     - the variance of the distribution of sample means is less than the variance of individuals\\
       $\sigma^2_M = \sigma^2 / N$  (???)
     - the shape of the distribution of sample means is approximately normal
*** Central Limit Theorem
    3 principles
     - The mean of the distribution of sample means is the same as the mean of the population
     - The standard deviation of the distrib. of sample means is the square root of the variance of
        the distribution of sample means,
       which is  $\sigma^2_M = \sigma^2 / N$  (???)
     - The shape of the distrib of sample means is approximtely normal if either
       -- (a) N >= 30
       -- (b) the shape of the population distribution is normal
 
** Correlation in R (L6)
*** Scatterplots and correlations in R (L6S1)
**** Example: Data from ImPACT (impact website)
     Main measures: verbal memory, visual memory, visual motor speed, reaction time, impulse control
     Data are available in the file STATS1.EX.02.TXT\\
     Notions learnt: functions
     - cor(X,Y) -> corr. coef
     - cor.test(X , Y) -> anlysis
     - cor(data.frame) -> matrix of corr. coef
     - plot( X,Y ) -> scatter plot
     - abline(lm( X ~Y )) -> regression line
*** Test/re-test reliability analysis in R (L6S2)
     Data from ImPACT but we assume each athlete passes the test at the beginning and then month later.\\
     If the two tests are correlated, it means we have a reliable instrument.
     - 40 athletes, test A, then test B
     Repeated measures on an idividual, how to structure the data set
     - Typical to add new columns (keeps 1 row per individual)\\
       Example: double the columns for two tests => call them .A and .B\\
         For a dataframe called 'impact.col':
         #+BEGIN_SRC R
         cor(impact.col$memory.verbal.A,impact.col$memory.verbal.B)  
         #+END_SRC

     - In R, sometimes it is better to add new rows \\
       Example: double the rows for the two tests => add a column named 'test', which equals 'A' or 'B'\\
       For a dataframe called impact.row:
         #+BEGIN_SRC R
         cor(impact.col$memory.verbal[impact.col$test == "A"],
             impact.col$memory.verbal[impact.col$test == "B"])
         #+END_SRC
       
     - Other function: =describe.by()=
       #+BEGIN_SRC R
       describe.by(impact.row, impact.row$test)
       #+END_SRC
       will perform a description by category (test='A' or 'test'=B).

* Introduction to Regression (L7)
  A statistical analysis used to predict scores on outcome variable,
  based on scores on one or more predictor variables
  - For example, predict how many runs a baseball player will score (Y)
    if we know the player's batting average (X)
** Regression Equation
   $Y = B_0 + B_1 X + e$ : Y is the dependant variable, X the independent var, e the error.
   $\hat{Y} = B_0 + B_1 X$ is the prediction\\
   $Y - \hat{Y} = e$ is the prediction error (*residual*)
   - The regression is a model
   - The goal is to produce more accurate predictors
   - Lesson: Examine residuals ! Scatterplot residuals with X:\\
     if i seee a realtionship bewteen X and the residuals, it means i have hetedasticity,
     and i should find another predictor.
** Estimation of the coefficients
   The values of the coefficients (B) are estimaded such that the model yields optimal predictions.
   - Minimize the residuals!
   - The sum of the squared (SS) residuals is minimized
   - SS.RESIDUALS = $\sum{ (\hat{hat}-Y)^2}$
   - ordinary least squares estimation
*** How to calculate B (unstandardized)
   - $B = r (SD_Y / SD_X)$  (this is the slope)
   - standardized regression coefficient : $\beta = r$
   - If X and Y are standardized, then $SD_X - SD_Y = 1$, hence $B=r$ (the correlation coef)
   - In R : =lm(X ~ Y)=
   
* NHST: A closer Look (L7S2)
** Recall
*** Principle: when starting a study, we assume the null hypothesis is true
    - H_0 =  null hypothesis, e.g r=0
    - H_A = alternative hypothese, e.g r>0
   Applied to regression, it means the slope is 0 or non-zero:
    - $H_0$ =  null hypothesis, e.g B=0
    - $H_A$ = alternative hypothese, B \not = 0: 
   NHST: Assume H_0 is true, then calculate the probability of 
     observing this data (equivalent, of this outcome) with these 
     characteristics, given that H_0 is true,  i.e:
     - Thus, p=P(D|H_0)
     - and if p < \alpha, reject H_0 else retain H_0
     \alpha is set by the experimenter, typically 0.05

#+begin_table
|             | Retain H_0   | Reject H_0 |
|-------------+--------------+------------|
| H_0 true    | p=(1-\alpha) | p=\alpha   |
|-------------+--------------+------------|
| H_0 false   | p = \beta    | p=1-\beta  |
|             | (1-Power)    | (Power)    |
#+end_table

POWER: the probabiliy of rejecting Hyp while we should (see lecture 9).

NHST can be applied to:
- r : is the correlation significantly different from 0?
- B : is the slope of regression line for X significantly different from 0?
** NHST for B
- t statistics:  t = B/SE
  where B is be the unstandardized regression coefficient, SE standard error
  (SE = \sqrt{SS.RESIDUALS} / (N-2))
  
** NHST: Problems!
*** Biased by N
   - p-value is based on t-value
   - t = B /SE 
   - SE = \sqrt{SS.RESIDUALS} / (N-2)
   Therefore, when N large, SE low, t large and low p-value. That puts you
   out of the extreme of the t distribution. This means small you can have 
   small effects to be significant.
 
*** Binary outcome (retain or not)
    retain or reject only. What if p=.06? Reject H_0
*** Null "model" is a weak hypothese
    Demonstrating that your model does better than NOTHING is not vert compelling
** Alternatives, just cited:
*** Effect size
    what is the magnitude of the effect? 
*** Confidence intervals
    sample statistics as "point estimates"
*** Model comparison
    propose multiple models and compare

**
* Multiple Regression (L8)
** Introduction Multi Regr.
   simple regr: 1 predictor (X), multiple regr: multiple predictors (X_1, X_2, X_3, ...)
*** Equation
   $\hat{Y}$ = B_0 + B_1 X_1 + B_2 Y_2 + ... + B_K Y_k
             = B_0 + \sum_{i=1..k}( B_i X_i) 
*** Model R and R^2
    R = multiple correlation coefficient
    - $R = r{\hat{Y} Y}$
   
*** Example
    -Outcome measure (Y) :  Faculty Salary
    - Predictors (X_1,X_2,X_3)
    - time since PhD (X1)
    - # of publications (X2)
    - Genders (X3, male=0, female=1)

Multiple regr. computed by R:\\
$\hat{Y} = 46911 + 1382(TIME) + 502(PUBS)$ + -3484(G)$
Tell the effects given an average on all other predictors.

*** Types ofmulti regr.

If the different predictors are orthogonal, results are easy.
Otherwise (predictors are correlated) then different methods will return different results
**** Standard method
  - All predictors are entered into the regr. equation at the same tile
  - Each predictor is evaluated in terms of what it adds to the prediction of Y 
    that is different from the predictability offered by the others.
  - Overlapping areas are assigned to R^2 but not to any individual B

**** sequential (aka hierarchical)
  -  


*** Interpretation

** Matrix Algebra (L8S2)
*** Basics
   - Multiplication : for matrices A(R,C) and B(R',C'), multiplication is possible when C=R'
   - Square symetric matrix : D = D^T
   - Inverses only exist  (but not necessarily) for square matrices
   - It is D^-1 such that  D x D^-1 = I
   - The determinant (square matrix). For a 2x2, |A|=a_11*a_22-a_12*a_21
*** From raw data matrix to correlation
   subjects as rows, variables as columns
    

** Multiple Regression  in R (L9S1)
*** Simple regression (1 predictor)
    We want to predict one outcome (Y), for example  physical endurance
    with one predictor (X), for example age. Use =lm()= (linear model)
    #+begin_src R
    > model.SR = lm( Y ~ X )
    > model.SR
 
*** MR  (2 predictor)
    The outcome is predicted by several predictors (X_1,X_2), for example  age, years engaged in active exercise (activeyears)\\
    A *linear combination* of predictors.    
#+begin_src R
    > model.MR = lm( Y ~ X1 + X2)
    > model.MR
    > summary(model.MR)
#+end_src

*** Standardized regression
   Use =scale()=
   For instance 
#+begin_src R
    > model.MR = lm( scale(Y) ~ scale(X1) + scale(X2))
#+end_src

* Mediation Analysis (L10)
** Mediation by regression approach
   - Think of a mediator
   For instance, 
   - X : predictor (could be an IV)
   - Y : outcome  (could be a DV)
   - M : mediator
   - Z : moderator
** Link to regression
   - if X and Y are correlated then we can use regressoin to predict Y from X
   Y = B_0 + B_1 X + e
   - if X and Y are correlated BECAUSE of hte mediator M, then (X \rightarrow M \rightarrow Y)
   Y = B_0 + B_1 M + e   and\\
   M = B_0 + B_1 X + e
  - Hence Y = B_0 + B_1 M + B_2 X + e
  A mediator variable M accounts for some or all of the relationship between X and Y
  (partial of full mediation)
** How to test for mediation  
Run three regression models
- lm (Y ~ X) : regress coeff. for X should be significant. This must be a significant predictor.
- lm (M ~ X) : regress coeff. for X should be significant
- lm ( Y ~ X_M) : regress coeff. for M should be significant, regress coeff. for X ?
  If regre. coeff for X approaches 0, this means full mediation

*** Example
Assume N=188
Participants surveyed asked to report
- Happiness (happy)
- Extraversion (extra)
- Diversity of life experiences (diverse) \rightarrow this is the mediator
 Assume all are scored on a 1-5
First 2 models:
- happy = 2.19 + .28 (extra)
- diverse = 1.63 + .28(extra)
For both, regr. coeff for X (extra) is statically significant, p < 0.05
Full model: happy = 1.89 + .22(extra) + .19(diverse)  
THis means : partial mediation

* Mediation: Path Analysis method (L10S2)
** Paths models
   - rectangles: Observed variables, aka manifest variables (X,Y,M) 
   - circles: unobserved variables (e)
   - triangles: constants
   - arrows: associations (more on these later)

** How to test for mediation  
The three regression eqs, now with labels
- lm (Y ~ X) : becomes Y = B_0 + cX +e
- lm ( Y ~ X+M) :becomes Y = B_0 + c'X + bM + e  
- lm (M ~ X) : becomes M = B_0 + aX +e

Another way to test for mediation is to use the *Sobel test*.
z = \frac{B_a * B_b}{\sqrt{B_a^2*SE_b^2 + B_b^2*SE_a^2}}
/Intuition:/ Then numerator ind

Principle: assume a null indirect effect (NHST), i.e the effect
of M in X->M->Y is zero.
The null hypothesis \Rightarrow B_a*B_b=0

* Moderation Analysis (L11)
  - Moderation Quick example (L11S1)
  - Details (L11S2)
  - Moderation Example 2 (L11S3)
** Moderation Example 1
   Mediation and moderation are quite different
   
   Moderation: 
   - X : psychological trait, measured by extraversion
   - Y : behavorial outcome, measured by hapiness
   - M : Mechanism, measured by diversity of life experience
   - Z : Moderator (ZAP! or ZING!), measured by socio-economic status (SES)
   Quick example:
   - Working Memory capacity (X)
   - SAT (Y)
   - Type of University (Z)
   There is generally a strong correlation between X and Y. However, in 
   some universities, the correlation is not observed. Reason: the sample
   is not representative. For instance, there are a lot of students with
   a high SAT score. Interpretation: Type of University moderates the
   relationship between WMC and SAT. 

   Moderation model
   - Y = B_0 + B_1X + B_2 Z + B_3 (X*Z) + e
  Regression Model in R
   - lm ( Y ~ X + Z + X*Z)
   We need to create a new column for (X*Z), call it *product*: this
   is a way to trick the general linear model (as it assumes only
   additive
   
   Example:
   - assume N=188 
   - SES : 1=high SES, 0=low SES
   - other values range from [1-5]

   - Results *before adding the product*:
   Y = B_0(1) + B_1(EXTRA) + B_2(SES)
   Y = 3.04 + 0.039(EXTRA) + 0.00(SES)
   - Interpretation:  extra-version is not a predictor of hapiness
   - Results *before after the product*:
   Y = B_0(1) + B_1(EXTRA) + B_2(SES) + B_3(PRODUCT)
   Y = 3.88 + -0.20(EXTRA) + -1.69(SES) + 0.47(PRODUCT)

   Consider: 
   - the case SES=0: EXTRA and HAPINESS are negatively correlated:
     3.88 - 0.20(EXTRA)
   
   - the case SES=1 implies a positive correlation:
     3.88 + -0.20(EXTRA) 

   Conclusion: SES moderates the relationship between extraversion and hapiness.
   Thus, the picture can change, litterally, when you consider a new variable.

* Moderation Analysis. Details: centering and dummy coding (L11S2)
** Centering predictors
   Centering means put in /deviation form/, i.e for each score X, X_c = X - M \\
   Why centering?
   - conceptual reason
   - statistical reason (the predictors X1 and X2 can be highly correlated with X_1*X_2)
** Run sequential regression (2 steps)
   - step 1: Main effects
   - step 2 : Moderation effect: Evaluate B for PRODUCT of \delta R^2 from Model 1 to Model 2
** Dummy coding
   a system to code categorical predictors in a regression analysis
   - For instance, three predictors C1, C2, C3 and 4 groups G1, ..., G4
   - The reference group, say 'Cog' is numbered 0,0,0 (for C_1,C_2,C_3 resp.)
     So, we add to ne normal columns, the 3 predictors

   | case | group     | DV | C1 | C2 | C3 |
   |------+-----------+----+----+----+----|
   |    1 | Cog       | 61 |  0 |  0 |  0 |
   |    2 | Soc       | 78 |  1 |  0 |  0 |
   |    3 | Neuro     | 47 |  0 |  1 |  0 |
   |    4 | Cog Neuro | 65 |  0 |  0 |  1 |
 
    Run a regression:
    Y = B_0 + B_1(C_1) + B_2(C_2) + B_3(C_3)
   

   |   | group         | Unstandardized coeff |           | Stand. coeff |     t |
   |   |               |        B (intercept) | Std Error |         Beta |       |
   |---+---------------+----------------------+-----------+--------------+-------|
   | 1 | Cog           |                93.30 |      6.49 |              | 14.36 |
   | 2 | Soc(C1)       |               -32.64 |     10.15 |        -.514 | -3.21 |
   | 3 | Neuro(C2)     |                10.19 |     11.55 |         .138 |   .88 |
   | 4 | Cog Neuro(C3) |               -23.18 |     10.52 |        -.351 | -2.20 |
  
    This gives the differences between the groups and the reference group.
            
        
* Moderation Example 2 (L11S3)
** Objectives: analysis of professor salaries
  - DV = salary
  - IVs : # of publications, Department (Psy, Socio, History)
  *Question* : does the department moderates the salary?
  (We know that salary is coralated with publications)
** Steps
  - center publications
  - define dummy coding for departments
    (Regular dummy coding with the Psych as the reference group)
  - create moderation terms
  - Run sequential regression

* Mediation and Moderation in R
** Mediation in R
   Write a script in R to test for mediation:\\
   Three regression analysis
   - Outcome = predictor
   - Predictor = Mediator
   - Outcome = predictor + Mediator
   Run three regression models
   - lm (Y ~ X) 
   - lm (X ~ M) 
   - lm (Y ~ X + M ) 
   If regre. coeff for X approaches 0, this means full mediation

* Students's t-test (L13)
  From multiple regression to t-test??
  The examples discussed in multiple regression were complicated,
  considering the limitations placed on final intrerpretation, e.g.
  - the slope for X is B, but if you add another variable, then the slope changes!
  In comparison, a simple controlled regression is more powerful because it
  puts forward causality.
  
** t-test and z-test
    Two means can be compared using t-test
    z = (observed - expected) / SE
    t = (observed - expected) / SE
*** When use z and t?
    - z: when comparing a sample mean to a population mean and the 
             standard deviation (SD) of the population is known
    - Single sample t : when comparing a sample mean to a population mean and the
             standard deviation (SD) of the population is not known
    - Dependent samples t (same people in different conditions, e.g score before and after training): 
             when evaluating the differences between two related samples       
    - Independent samples t (two groups of people): 
             when evaluating the differences between two independent samples
*** Notation
    greek letters for population
    - \sigma : SD for population
    - \mu : mean for population
    Sample
    - SD :
    - M : 
*** p-value for z and t
    Exact p-value depends on 
    - directional or non-directional test?
    -df (Degrees of freedom)
      different t-distributions for different sample sizes
    One z distribution but a family of t distributions    
    
    |                   | df                |
    |-------------------+-------------------|
    | z                 | N/A               |
    | t (single sample) | N-1               |
    | t (dependent)     | N-1               |
    | t (independent)   | (N_1-1) + (N_2-1) |
    |                   |                   |

    Compare a sample mean to a population mean
    t = (M-\mu) / SE_M
    SE_M^2 = SD_M^2/ N   => SE_M = SD / \sqrt(N)
    SD^2 = \sum_i (X_i-M) / (N-1) = SS/df = MS

*** Single sample t

** dependent and independent t-tests (L13S2)
   - dependent (the same group). 
     Example: test if a people smoke less after a treatment. 
     Variables:
     number of cigarettes smoked before and after the treatment.
     - calculate the mean of differences (M_D).
     - calculate the deviation squares (D-M_D)^2
     
   - Independent means t
     t=(M_1-M_2) / SE_difference
     SE^2_difference = SE^2_{M_1} + SE^2_{M_2}
     SE^2_{M_1} = SD^2_pooled / N_1
     SE^2_{M_2} = SD^2_pooled / N_2
     SD^2_pooled = df_1/df_total (SD^2_1) + df_2/df_total(SD^2_2)
     
     Notice that this is jsut a weighted average of the sample variance.

* ANOVA (L14S1)
** The general linear model (GLM)
*** Characteristics of GLM
   - Linear: pairs of variables are assumed to have linear relations
   - Additive: if one set of variables predict another variable, the
     effects are thought to be additive
   BUT! This does not preclude testing non-linear or non-additive effects.\\
   GLM can accomodate such tests, for example:
   - transformation of variables
   - moderation analysis
*** GLM Example
   - simple Regression : Y = B_0 + B_1 X_1 + e
     with Y = faculty salary, X_1 = years since PhD
   - identical to one-way ANOVA : Y = B_0 + B_1 X_1 + e
     with Y = faculty salary, X_1 = gender 
     (use if categorical predictor instead instead of continous predictor)

*** ANOVA
    - ANOVA is a special case of multiple regression where predictors are orthogonal (are not correlated).
    -Appropriate when the predictors (IVs) are all categorical and the
    outcome (DV) is continuous.
    - Most common applications is to analyze data from randomized experiments

    t-test allows to compare 2 means, but ANOVA allows for more than 2.

*** F-test
    The test statistic is the F-test
    - F = systematic variance / unsystematic variance
    - The F-test has a family of F distributions

* One-way ANOVA (L14S2)
** Working Memory (WM) Example
   - IV : # of training sessions (8,12,17,19)
   - DV : gain score (post-pre)

  WM training (data)
 
 | subject | condition | pre | post | gain |
 |---------+-----------+-----+------+------|
 |         |           |     |      |      |
 |         |           |     |      |      |

*** F ratio
   The ratio of the mean squares
   - F = syst. var / unsystematic var.
   - F = between-groups var / within-groups var.
   - F = MS_between / MS_within
   - F = MS_A / MS_{S/A}
   A: group , S: subject, S/A : "S within A"

   - MS_A = SS_A / df_A
   - MS_S/A = SS_S/A / df_S/A


   Sum of squares between groups (how much are the group means vary from each other):
   SS_A = n \sum ( Y_j - Y_T)^2
   - Y_j are treatment means
   - Y_T is the grand mean
  
   Unsystematic var. = error variance = the variance within one group
    The variance of individuals wrt the group mean (people were treated
    in the same condition but perform differently).

    SS_S/A = \sum( Y_ij - Y_j)^2
    - Y_ij are individual scores
    - Y_j are the treatment means

    Degree of freedom
    - df_A = a - 1
    - df_S/A = a (n-1)
    - df_total = N - 1

** Summary Table
  
    | Source | SS                   | df     | MS              | F           |
    |--------+----------------------+--------+-----------------+-------------|
    | A      | n \sum (Y_j - Y_T)^2 | a-1    | SS_A / df_A     | MS_A/MS_S/A |
    | S/A    | \sum (Y_ij - Y_j)^1  | a(n-1) | SS_S/A / DF_S/A | ----        |
    | TOTAL  | \sum(Y_ij - Y_T)^2   | N-1    | ----            | ----        |

**

*** Effect size
   
    \eta^2 : percentage of variance explained in the outcome variable (=dependent variable)
    Exactly the same as R^2 in multiple regression
    
    So, for our example : \eta^2 = SS_A / SS_total = 50 /75 = .66  (unusually large)
    
    Assumptions
    - DV is continuous
    - DV is normally distributed
    - Homogeneity of variance : within-group variance is equivalent for all groups (test it with Levene's test)

* Factorial ANOVA (L13S3)

** Two IVs (tratements)
   
    Example
    - IV : driving driving difficulty (easy, difficuult)
    - IV : conversation difficulty (none, easy, difficult)
    - DV : errors made in driving simulator

    Three hypotheses can be tested:
    - More errors in the difficult simulator ?
    - More errors when conversation more difficults?
    - More errors due to the interactoins of these factors?

    Therefore, the subjects were assigned to 1 of 6 groups (2x3 values)

    F_A = MS_A / MS_S/AB
    F_B = MS_B / MS_S/AB
    F_AxB = MS_AxB / M_S/AB

*** Follow-up tests
   Main effects
   - post-hoctests
   Interaction
   - Analysis of simple effects : conduct a series of one-way ANOVAs
     comparing high and low spans at each level of the other IVs
   Effect size  
 




* R
** Install packages
   From console: 
#+BEGIN_SRC R
   > install.package("pschy")
   > library(psych)
   > search() // list loaded pacakges
#+END_SRC

** Script
   Example: wine testing (file )
#+BEGIN_SRC R
   Ratings <- read.table("stats1_ex01.txt",header = T) # 1st line = row names
   > class(ratings)
     [1] "data.frame"
   > names(ratings)
   [1] "RedTruck" "WoopWoop" "HobNob"   "FourPlay"
   hist(ratings$RedTruck)
   # --> plots histo
   layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
   hist(ratings$RedTruck, xlab = "Ratings", ylab="Number", main="RedTruck")
   hist(ratings$HobNob, xlab = "Ratings", ylab="Number", main="HobNob")
   hist(ratings$FourPlay, xlab = "Ratings", ylab="Number", main="FourPlay")
   hist(ratings$WoopWoop, xlab = "Ratings", ylab="Number", main="WoopWoop")
   describe(ratings)  # from the 'psych' package, 
   summary(ratings)
#+END_SRC 
