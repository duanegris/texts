

%---------------------------- R E C H E R C H E --------------------------------
\section{Activités de recherches}

\paragraph{Résumé}
\textit{Mes   thèmes    de   recherche   concernent    le   \emph{parallélisme},
  essentiellement  sur  des architectures  de  type  cluster, \emph{grilles}  ou
  \emph{cloud}.  Ma  thèse  de  doctorat  avait  pour  objet  l'écriture  et  la
  transformation de  programmes parallèles  à l'aide d'un  langage formel.  A la
  suite de cette approche formelle, je me suis investi sur un problème réel dans
  le domaine  de la géophysique,  nécessitant la conception et  le développement
  d'applications parallèles. Lorsqu'ont émergé les grilles, j'ai étudié certains
  problèmes nouveaux que posaient ces systèmes hétérogènes, et la façon dont des
  applications  pouvaient  y être  déployées.  Mon  travail  sur les  grilles  a
  concerné  l'évaluation  et  l'amélioration  des  performances  des  programmes
  parallèles dans ce  contexte, puis l'amélioration des  intergiciels pour mieux
  prendre en charge  les programmes parallèles. L'utilisation  des principes des
  systèmes pair-à-pair pour la  découverte et l'auto-organisation des ressources
  ainsi que des  mécanismes de tolérance aux pannes par  réplication des calculs
  ont  été  proposés.   Parallèlement  aux   évaluations  sur  des  cas  et  des
  plate-formes réelles, je travaille à la simulation de programmes distribués en
  contribuant  à l'outil  \textsc{SimGrid}.  Je  m'intéresse particulièrement  à
  l'extension du simulateur pour permettre  la simulation de programmes MPI sans
  modification du  code source.  Enfin,  mes recherches actuelles  sont tournées
  vers les problèmes d'allocation des ressources et d'ordonnancement des tâches,
  dans le but de proposer aux clients de bons compromis performance/prix sur des
  plates-formes virtualisées comme celles fournies par les clouds IaaS.}\\

Ces différents  aspects sont détaillés  ci-après, puis  suivent la liste  de mes
publications, mon  activité d'animation  scientifique, mes participations  à des
encadrements, et mes perspectives de recherches.

\subsection{Détails des thèmes de recherche}


\subsubsection{Spécification de programmes data-parallèles}

Ma  thèse  de   doctorat  \cite{icps-1997-4}  a  porté  sur   la  définition  et
l'utilisation  d'un   langage  formel  baptis\'e   Pei~\footcite{Violard92}.  Ce
formalisme permet la description de  programmes pour des ordinateurs parallèles,
dans un  modèle de  programmation de type  \emph{parallélisme de  données}. Nous
avons montré comment  ce formalisme pouvait être utilisé pour  raisonner sur les
programmes et les transformer  en nouveaux programmes sémantiquement équivalents
ou    raffinés   \cite{icps-1994-46,icps-1995-1,icps-1997-3,icps-1996-2}.     Le
deuxième volet du  travail a consisté à proposer des  méthodes pour traduire ces
énoncés  formels   vers  des  langages  parallèles   cibles  comme  \textit{High
  Performance Fortran}  ainsi que les logiciels  permettant les transformations,
le  contrôle de  validité  des programmes  ainsi que  les  compilateurs pour  la
traduction%
\footnote{\url{http://icps.u-strasbg.fr/pei/PEI_SUMMARY/langage.htm}}.


\subsubsection{Parallélisation pour la géophysique}

%..............................................................................|80
Après cette expérience d'approche formelle  d'un modèle de programmation pour le
parallélisme, je me  suis plongé dans un travail concret  de parallélisation sur
des codes scientifiques. Je me suis investi en particulier dans la conception et
le développement d'outils logiciels pour  la géophysique. L'enjeu est de pouvoir
calculer  une  tomographie  sismique  globale en  ondes  de  volumes  permettant
d'améliorer un modèle de  vitesses des ondes sismiques, et de  là en déduire des
propriétés  géologiques   à  l'intérieur   de  la   Terre.  L'ensemble   de  ces
outils\footnote{\url{http://renass.u-strasbg.fr/ray2mesh}}  conçus   lors  d'une
thèse en collaboration  avec l'Institut de Physique du Globe  de Strasbourg (UMR
CNRS-UdS 7516) permet de combiner et d'enchaîner des traitements sur des données
géophysiques  extrêmement  volumineuses. Le  travail  s'est  concrétisé par  une
tomographie  utilisant la  totalité des  données (sismogrammes)  acquises depuis
1965  par les  réseaux  de  surveillances sismiques  à  travers  le monde.  Pour
répondre à cet objectif les applications ont été conçues pour s'exécuter sur des
architectures  parallèles,  et  ont  été testées  sur  des  configurations  très
différentes allant de  la machine parallèle à des réseaux  de station de travail
en  passant par  des  grilles de  calcul (cf.  paragraphe  suivant) à  l'échelle
nationale~\cite{icps-2005-146,icps-2007-184}.


\subsubsection{Grilles de calcul}

%..............................................................................|80
L'exploitation de telles applications  scientifiques pose des problèmes concrets
quant au  choix de  la machine  cible pour  l'exécution. Un  type d'architecture
nouveau a émergé  ces dernières années grâce aux technologies  qui permettent de
fédérer  efficacement   des  ressources   de  calcul   provenant  d'institutions
différentes.  Cette alternative  a été  popularisée~\footcite{Foster97,Foster98}
sous  l'appellation de  \emph{grilles}. J'ai  porté en  2001 un  projet intitulé
\textit{Transformations  et  Adaptations de  programmes  pour  la Grille}  (TAG)
accepté dans le  cadre de l'Action Concertée Incitative (ACI)  Grid du Ministère
de  la  Recherche  (2002--2005).  Le   projet  visait  l'étude  du  comportement
d'applications  scientifiques,  essentiellement des  programmes  MPI~\footnote{%
  Message Passing Interface  définit une bibliothèque de  fonctions permettant à
  des processus  d'échanger des  messages. C'est  devenu le  standard \textit{de
    facto}  pour les  programmes  parallèles exécutés  sur  des architectures  à
  mémoire distribuée.}  sur les  grilles. Deux catégories  de problèmes  ont été
traitées dans ce  projet. D'une part, déterminer  quelles \emph{performances} on
peut  espérer   d'une  application  exécutée  sur   des  ressources  hétérogènes
distribuées  à large  échelle  géographique  et ce  qu'on  peut  faire pour  les
améliorer.   D'autre part,  travailler sur  la couche  \emph{intergicielle} pour
masquer la complexité  de ces systèmes répartis et améliorer  la prise en charge
des applications.


\paragraph{Performances} 

Pour améliorer  les performances,  une partie  de mon travail  a été  l'étude de
techniques  d'équilibrage de  charge  statiques ou  dynamiques.   En effet,  les
découpages  des données  et  la distribution  de la  charge  dans de  nombreuses
applications parallèles font l'hypothèse que les processeurs et les réseaux sont
homogènes.  Pour  optimiser   le  temps  de  calcul  d'un   ensemble  de  tâches
indépendantes exécutées  sur des processeurs  hétérogène, il faut  déterminer la
meilleure  quantité de  données distribuées  à chacune.  Nous avons  proposé des
algorithmes pour déterminer statiquement  de telles distributions optimales. Ces
algorithmes ont  été testés  sur de vraies  applications, dont  l'application de
géophysique décrite précédemment~\cite{icps-2002-62,icps-2003-75,icps-2004-125}.
Nous avons également comparé cette approche statique à l'approche dynamique dans
laquelle le  maître distribue un  nouveau bloc de travail  à un esclave  dès que
celui-ci le demande : l'équilibrage de  charge se fait alors naturellement selon
le  rythme de  calcul des  esclaves. La  difficulté dans  cette approche  est de
déterminer la taille du  bloc à envoyer: en dessous de la taille  idéale, il y a
des aller-retours inutiles,  tandis qu'au dessus, les esclaves  ne finissent pas
simultanément.  En  revanche,  cette  stratégie  possède  l'avantage  majeur  de
s'adapter aux variations de charge.


\paragraph{Intergiciel}

Les travaux  précédents se sont  largement appuyés sur  l'expérimentation réelle
sur des grilles construites  avec le logiciel Globus. La prise  en charge de nos
applications par cet intergicel ne correspondant  pas à nos attentes, nous avons
proposé un  nouveau type  d'intergiciel spécialisé pour  MPI. Parmi  les lacunes
observées  pour  l'exécution des  programmes  parallèles  figurent l'absence  de
mécanisme de co-allocation  de ressources sur différents sites,  la détection de
la disponibilité des  ressources au moment précis de l'exécution,  la gestion de
binaires multiples pour chaque systèmes, la difficulté d'accéder aux fichiers de
données et programmes, et l'absence de  détection des pannes et de tolérance aux
pannes.  Nous  avons  développé  {\pmpi}\footnote{~\url{http://www.p2pmpi.org}},
pour pallier ces lacunes. {\pmpi} comprend  à la fois la couche intergicielle et
la  bibliothèque  de  communication  permettant  de  développer  des  programmes
parallèles  à passage  de messages.  Cette  dernière est  une implémentation  de
MPJ\footnote{Message Passing for  Java: adaptation de MPI  pour Java}qui intègre
la notion  de tolérance aux pannes.  Les applications sont des  programmes Java,
beaucoup  plus faciles  à déployer  dans un  environnement hétérogène.   Pour la
couche  intergicielle, nous  reprenons les  principes des  systèmes pair-à-pair:
chaque machine démarrée avec {\pmpi} devient un pair susceptible de partager son
CPU  ou  d'utiliser  ceux  des  autres.  Cette  approche  confère  autonomie  et
robustesse aux applications. L'autonomie provient de la possibilité de découvrir
dynamiquement un ensemble de pairs disponibles  à ce moment précis pour exécuter
un programme parallèle.  On construit ainsi dynamiquement  une ``plate-forme'' à
chaque demande d'exécution. La façon de choisir les pairs les plus adaptés parmi
ceux disponibles dépend de plusieurs critères, dont la latence réseau qui sépare
la machine  qui fait  la requête  des pairs  candidats, la  présence ou  non des
données  nécessaires dans  les  caches  des pairs  distants,  et  le souhait  de
l'utilisateur de concentrer ou non les processus sur le minimum de machines.

Dans de tels systèmes, les pannes sont fréquentes. Or, lors d'un calcul, la 
panne d'un des participants provoque l'arrêt de l'application. Pour diminuer le 
risque de panne, nous avons proposé une solution jamais expérimentée dans ce 
contexte qui est la réplication des calculs. L'utilisateur décide du taux de 
redondance de l'exécution de chaque processus, sur des machines différentes, et 
le système gère de manière transparente la cohérence de l'exécution. En cas de 
panne de l'un des processus, l'application peut poursuivre son exécution tant 
qu'il subsiste au moins une copie de ce processus de calcul.\\


La conception de {\pmpi}  à été décrite dans~\cite{icps-2007-182,icps-2005-155}.
Nous  avons aussi  démontré  sa  capacité à  prendre  en  charge l'exécution  de
programmes    parallèles    sur     plusieurs    centaines    de    processeurs~
\cite{icps-2008-193}. La  thèse de  Choopan Rattanapoka~\footcite{icps-2008-208}
présente l'ensemble des résultats. {\pmpi} est  aussi un support pour l'étude de
la  tolérance aux  pannes. Nous  avons étudié  le mécanisme  de réplication  des
calculs que  nous proposons~\cite{icps-2007-185} en montrant  comment déterminer
un taux  optimal de réplication~\cite{icps-2009-217}  puis en faisant  une étude
quantitative    du    coût    de    la   réplication    et    des    temps    de
reprise~\cite{icps-2009-214}.

Enfin,  je  me   suis  attaché  à  valider  notre  proposition   sur  de  vraies
applications.Deux collaborations  ont abouties dans  ce cadre.  En 2007  et 2008
nous avons aidé des collègues du LSIIT (Pierre Gançarski), dans le domaine de la
fouille de  données, à paralléliser leur  méthode d'apprentissage non-supervisée
pour le clustering~\cite{icps-2008-188}.  En 2008  et 2009, nous avons collaboré
avec  des  collègues de  SUPELEC  (Virginie  Galtier  et Stéphane  Vialle)  pour
comparer  les implantations  parallèles de  la méthode  d'apprentissage Adaboost
dans    deux    modèles    de    programmation    différents    (JavaSpace    et
MPJ)~\cite{icps-2009-219}.


\subsubsection{Simulation}
\label{sc:simulation}

Les  expérimentations  menées  dans  notre  travail  sur  les  grilles  ont  été
fastidieuses. L'apparition  de l'outil  Grid'5000 a considérablement  élargi les
possibilités d'expérimentation dans un environnement réel, tout en permettant un
protocole  expérimental plus  rigoureux car  l'utilisateur peut  sélectionner le
matériel  voulu,  et installer  le  système  d'exploitation  de son  choix.   Le
caractère  reproductible des  expériences a  donc été  considérablement amélioré
avec Grid'5000, mais reste néanmoins imparfait,  car le réseau reliant les sites
ainsi que les  clusters sont régulièrement renouvelés.   Les expériences doivent
donc se succéder  sur une période relativement courte pour  obtenir une série de
résultats comparables,  alors que  les difficultés  pratiques pour  déployer une
expérience  pleinement  fonctionnelle  sur   l'outil  au  moment  souhaité  sont
réelles. La simulation  présente face à ce problème un  grand intérêt. Elle peut
permettre  de tester  de  nombreux scénarios,  et de  ne  faire des  expériences
réelles que pour les cas les plus intéressants.\\

\textsc{SimGrid}~\footcite{Casanova08} est  un projet important dans  le paysage
de  la  recherche académique  sur  la  simulation  des systèmes  distribués.  Le
logiciel permet de décrire l'ensemble des ordinateurs connectés et le réseau les
interconnectant,  ainsi  que  les  opérations  de  calcul  et  de  communication
survenant au sein  d'une application, et d'en faire une  simulation à évènements
discrets.  Le   déroulement  d'une   application  est   décrit  à   travers  une
\emph{interface} au  simulateur, c'est-à-dire une  API fournie pour  décrire ces
opérations de calcul et de communication.\\

Ce logiciel a été soutenu entre autres par le projet ANR USS-SimGrid%
\footnote{\url{http://uss-simgrid.gforge.inria.fr/}} 
\footnote{Labelisé projet \textit{phare} par l'ANR.}
(2009-2011), puis aujourd'hui par le projet SONGS%
\footnote{\url{http://infra-songs.gforge.inria.fr/}} (2012-2015). Je participe à
ces  projets,  dont  l'objectif  est  d'étendre  les  capacité  de  l'outil.  Il
n'existait pas par exemple, avant  le projet Uss-Simgrid, d'interface permettant
de simuler les programmes parallèles à passage de messages (ses deux principales
API proposaient alors  des communications point à point  bloquantes). Nous avons
proposé une interface  pour simuler des programmes MPI. J'ai  réactivé un effort
fait dans ce sens à l'université d'Hawaï (Mark Stillwell et Henri Casanova) mais
inachevé,  baptisée SMPI.  Ce  travail, poursuivi  par  Pierre-Nicolas Clauss  a
abouti  à  une  version  désormais   livrée  avec  SimGrid.   La  conception  et
l'évaluation  de  SMPI  ont  fait  l'objet d'une  publication  à  la  conférence
IPDPS~\cite{icps-2011-224}.  Plusieurs  perspectives   sont  ouvertes  avec  cet
outil.  On  peut extrapoler  l'exécution  d'un  programme  sur une  machine  qui
n'existe pas encore, dont  on ne donne que la description.  Ceci peut servir par
exemple à des fins de dimensionnement d'un cluster. Cela peut être utile dans de
nombreuses autres  situations, comme  l'enseignement, où  une machine  de bureau
peut servir  à simuler  un cluster  ou un système  distribué. On  peut également
imaginer  dans le  futur  continuer  des exécutions  en  simulation après  avoir
capturé la trace d'un programme réellement exécuté (avec des outils de profiling
existants) et en injectant cette trace dans le simulateur.

